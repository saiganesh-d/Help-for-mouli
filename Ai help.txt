import psycopg2
import nltk
import string
import os
from collections import Counter
from nltk.corpus import stopwords, wordnet
from fuzzywuzzy import process
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import pandas as pd
import spacy

# Load NLP models
nltk.download("stopwords")
nltk.download("wordnet")
nlp = spacy.load("en_core_web_sm")
stop_words = set(stopwords.words("english"))
punctuation = set(string.punctuation)

# ‚úÖ Step 1: Fetch CVE Data from PostgreSQL
def fetch_cve_data():
    conn = psycopg2.connect(
        dbname="your_db",
        user="your_user",
        password="your_password",
        host="your_host",
        port="your_port"
    )
    cursor = conn.cursor()

    query = """
    WITH applicable_cpes AS (
        SELECT 
            cpe_match.match_criteria_id,
            STRING_AGG(DISTINCT cpeentry.cpe_name, ', ') AS applicable_cpe_names
        FROM cpe_match
        JOIN cpeentry ON cpe_match.match_criteria_id = cpeentry.cpe_name_id
        GROUP BY cpe_match.match_criteria_id
    )

    SELECT 
        cve.id AS cve_id,
        cve.source_identifier,
        cve.published AS published_date,
        cve.last_modified AS last_modified_date,
        cve.vuln_status,
        cvss.base_score,
        cvss.base_severity AS severity,
        cvex.expression AS match_criteria_expression,
        ac.applicable_cpe_names,
        STRING_AGG(DISTINCT cve_references.ref_url, ', ') AS references
    FROM cve
    LEFT JOIN cvssdata cvss ON cve.id = cvss.cve_id
    LEFT JOIN cveconfigureexpressions cvex ON cve.id = cvex.cve_id
    LEFT JOIN applicable_cpes ac ON cvex.expression LIKE '%' || ac.match_criteria_id || '%'
    LEFT JOIN cvereferences cve_references ON cve.id = cve_references.cve_id
    GROUP BY cve.id, cve.source_identifier, cve.published, cve.last_modified, cve.vuln_status, 
             cvss.base_score, cvss.base_severity, cvex.expression, ac.applicable_cpe_names;
    """

    cursor.execute(query)
    data = cursor.fetchall()
    cursor.close()
    conn.close()
    
    columns = [
        "cve_id", "source_identifier", "published_date", "last_modified_date", "vuln_status",
        "base_score", "severity", "match_criteria_expression", "applicable_cpe_names", "references"
    ]
    df = pd.DataFrame(data, columns=columns)
    df.to_csv("cve_data_with_cpe.csv", index=False)
    return df

# ‚úÖ Step 2: Generate Security Terms from CVE Data
def generate_security_terms(df):
    word_list = []
    for text in df["source_identifier"].fillna(""):
        words = text.lower().split()
        words = [w for w in words if w.isalnum() and w not in stop_words and w not in punctuation]
        word_list.extend(words)
    common_terms = [word for word, _ in Counter(word_list).most_common(500)]
    return common_terms

# ‚úÖ Step 3: Query Enhancements
def correct_query(query, common_terms):
    words = query.split()
    corrected_words = [process.extractOne(word, common_terms)[0] for word in words]
    return " ".join(corrected_words)

def expand_query(query):
    words = query.split()
    expanded_words = []
    for word in words:
        synonyms = []
        for syn in wordnet.synsets(word):
            for lemma in syn.lemmas():
                synonyms.append(lemma.name())
        expanded_words.append(word)
        expanded_words.extend(list(set(synonyms))[:3])
    return " ".join(expanded_words)

def extract_entities(query):
    doc = nlp(query)
    entities = {"products": [], "versions": []}
    for ent in doc.ents:
        if ent.label_ in ["ORG", "PRODUCT"]:
            entities["products"].append(ent.text)
        elif ent.label_ == "CARDINAL":
            entities["versions"].append(ent.text)
    return entities

# ‚úÖ Step 4: AI-Powered Search with FAISS
def load_faiss_index(df):
    model = SentenceTransformer("all-MiniLM-L6-v2")
    cve_descriptions = df["source_identifier"].fillna("").tolist()
    cve_ids = df["cve_id"].tolist()
    cve_vectors = model.encode(cve_descriptions, convert_to_numpy=True)

    dimension = cve_vectors.shape[1]
    faiss_index = faiss.IndexFlatL2(dimension)
    faiss_index.add(cve_vectors)
    faiss.write_index(faiss_index, "cve_faiss.index")
    return model, faiss_index, cve_ids, cve_descriptions

# ‚úÖ Step 5: AI-Powered Search with Query Understanding
def enhanced_search_cve(query, model, faiss_index, cve_ids, cve_descriptions, common_terms):
    corrected_query = correct_query(query, common_terms)
    expanded_query = expand_query(corrected_query)
    entities = extract_entities(expanded_query)

    print(f"üîç Corrected Query: {corrected_query}")
    print(f"üîç Expanded Query: {expanded_query}")
    print(f"üîç Extracted Entities: {entities}")

    query_vector = model.encode([expanded_query], convert_to_numpy=True)
    distances, indices = faiss_index.search(query_vector, 5)

    results = [
        {"cve_id": cve_ids[idx], "description": cve_descriptions[idx], "distance": distances[0][i]}
        for i, idx in enumerate(indices[0])
    ]

    return results

# ‚úÖ Step 6: Initialize & Run Search
print("üöÄ Fetching CVE data...")
df_cve = fetch_cve_data()

print("üöÄ Generating security terms...")
common_terms = generate_security_terms(df_cve)

print("üöÄ Loading AI model & FAISS index...")
model, faiss_index, cve_ids, cve_descriptions = load_faiss_index(df_cve)

query = "Windwos kernal RCE flaws in OpenSSL 3.0"
results = enhanced_search_cve(query, model, faiss_index, cve_ids, cve_descriptions, common_terms)

print("\nüîπ Top CVE Results:")
for res in results:
    print(f"{res['cve_id']} - {res['description']} (Score: {res['distance']:.4f})")
import faiss
import os
import numpy as np
from sentence_transformers import SentenceTransformer

def save_faiss_index_and_model(df):
    """Encodes descriptions and saves FAISS index & model for future reuse."""
    model = SentenceTransformer("all-MiniLM-L6-v2")

    # Convert CVE descriptions into embeddings
    cve_descriptions = df["source_identifier"].fillna("").tolist()
    cve_ids = df["cve_id"].tolist()
    cve_vectors = model.encode(cve_descriptions, convert_to_numpy=True)

    # Create FAISS index
    dimension = cve_vectors.shape[1]
    faiss_index = faiss.IndexFlatL2(dimension)
    faiss_index.add(cve_vectors)

    # Save FAISS index
    faiss.write_index(faiss_index, "cve_faiss.index")

    # Save CVE IDs for lookup
    np.save("cve_ids.npy", np.array(cve_ids))
    np.save("cve_descriptions.npy", np.array(cve_descriptions, dtype=object))

    # Save AI model
    model.save("all-MiniLM-L6-v2-local")

    print("‚úÖ FAISS index and model saved successfully!")

# Run only once to save the model
save_faiss_index_and_model(df_cve)


def load_faiss_index_and_model():
    """Loads precomputed FAISS index and AI model for quick search."""
    if not os.path.exists("cve_faiss.index"):
        raise FileNotFoundError("FAISS index not found! Run save_faiss_index_and_model first.")

    # Load FAISS index
    faiss_index = faiss.read_index("cve_faiss.index")

    # Load CVE IDs and descriptions
    cve_ids = np.load("cve_ids.npy", allow_pickle=True).tolist()
    cve_descriptions = np.load("cve_descriptions.npy", allow_pickle=True).tolist()

    # Load AI model from local storage
    model = SentenceTransformer("all-MiniLM-L6-v2-local")

    print("‚úÖ FAISS index and model loaded successfully!")
    return model, faiss_index, cve_ids, cve_descriptions

# Call this in every new session instead of rebuilding the model
model, faiss_index, cve_ids, cve_descriptions = load_faiss_index_and_model()

def enhanced_search_cve(query, model, faiss_index, cve_ids, cve_descriptions, common_terms):
    corrected_query = correct_query(query, common_terms)
    expanded_query = expand_query(corrected_query)
    entities = extract_entities(expanded_query)

    print(f"üîç Corrected Query: {corrected_query}")
    print(f"üîç Expanded Query: {expanded_query}")
    print(f"üîç Extracted Entities: {entities}")

    query_vector = model.encode([expanded_query], convert_to_numpy=True)
    distances, indices = faiss_index.search(query_vector, 5)

    results = [
        {"cve_id": cve_ids[idx], "description": cve_descriptions[idx], "distance": distances[0][i]}
        for i, idx in enumerate(indices[0])
    ]

    return results

# Run AI-powered search
query = "Windwos kernal RCE flaws in OpenSSL 3.0"
results = enhanced_search_cve(query, model, faiss_index, cve_ids, cve_descriptions, common_terms)

print("\nüîπ Top CVE Results:")
for res in results:
    print(f"{res['cve_id']} - {res['description']} (Score: {res['distance']:.4f})")
import numpy as np

def encode_in_batches(text_list, batch_size=256):
    """
    Encode text in batches to avoid memory issues and speed up processing.
    """
    embeddings = []
    for i in range(0, len(text_list), batch_size):
        batch = text_list[i:i+batch_size]
        batch_embeddings = model.encode(batch, convert_to_numpy=True, show_progress_bar=True)
        embeddings.append(batch_embeddings)
    
    return np.vstack(embeddings)  # Stack all batch results

# Encode CVE descriptions efficiently
cve_descriptions = df["source_identifier"].fillna("").tolist()
cve_vectors = encode_in_batches(cve_descriptions)

print("‚úÖ Encoding completed!")

import torch
print("Is CUDA Available? ", torch.cuda.is_available())
print("CUDA Device:", torch.cuda.get_device_name(0))


pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

import torch
from sentence_transformers import SentenceTransformer

# Load model and move to GPU
device = "cuda" if torch.cuda.is_available() else "cpu"
model = SentenceTransformer("all-MiniLM-L6-v2").to(device)

print(f"‚úÖ Running on: {device.upper()}")


import numpy as np

def encode_in_batches(text_list, batch_size=256):
    """
    Encode text in batches to avoid memory issues and speed up processing.
    """
    embeddings = []
    for i in range(0, len(text_list), batch_size):
        batch = text_list[i:i+batch_size]
        batch_embeddings = model.encode(batch, convert_to_numpy=True, show_progress_bar=True)
        embeddings.append(batch_embeddings)
    
    return np.vstack(embeddings)  # Stack all batch results

# Encode CVE descriptions efficiently
cve_descriptions = df["source_identifier"].fillna("").tolist()
cve_vectors = encode_in_batches(cve_descriptions)

print("‚úÖ Encoding completed!")


def enhanced_search_cve(query, model, faiss_index, cve_ids, cve_descriptions, common_terms):
    corrected_query = correct_query(query, common_terms)
    expanded_query = expand_query(corrected_query)
    entities = extract_entities(expanded_query)

    print(f"üîç Corrected Query: {corrected_query}")
    print(f"üîç Expanded Query: {expanded_query}")
    print(f"üîç Extracted Entities: {entities}")

    query_vector = model.encode([expanded_query], convert_to_numpy=True)
    distances, indices = faiss_index.search(query_vector, 5)

    results = [
        {"cve_id": cve_ids[idx], "description": cve_descriptions[idx], "distance": distances[0][i]}
        for i, idx in enumerate(indices[0])
    ]

    return results

# Run AI-powered search
query = "Windwos kernal RCE flaws in OpenSSL 3.0"
results = enhanced_search_cve(query, model, faiss_index, cve_ids, cve_descriptions, common_terms)

print("\nüîπ Top CVE Results:")
for res in results:
    print(f"{res['cve_id']} - {res['description']} (Score: {res['distance']:.4f})")
